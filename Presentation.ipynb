{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from validate import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "- built on top of Numpy array, which means they are fast\n",
    "- a Data Frame is a container of Series\n",
    "- all Pandas data structures are value mutable (can be changed)\n",
    "- all _except_ for Series all are size mutable\n",
    "  - Series is size immutable\n",
    "  \n",
    "I've broken up the notebook into four sections\n",
    "\n",
    "1. Instantiation: Which simply goes over different ways to create a series or data frame object. This is purposefully basic to illustrate how there are many ways to import your data.\n",
    "2. Accessing Data: Which walks through different operations you can apply on a data frame. There is no coherent big picture in this section and simply walks through some useful commands.\n",
    "3. Working With A Dataset: This is the main section of this notebook. We'll be using the Titanic training dataset as a source of data to explore. Some of the python code will be provided, while other pieces will be blank and ask you to figure out how to complete each task.\n",
    "4. `eval` and `query`: We'll walk through the usages of these two performant methods.\n",
    "5. Challenges: If you're experienced with Pandas or just want to work through problems, skip to this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Instantiation Series](#Instantiation-Series)\n",
    "- [Accessing Data Series](#Accessing-Data-Series)\n",
    "- [Instantiation Data Frame](#Instantiation-Data-Frame)\n",
    "- [Accessing Data Data Frame](#Accessing-Data-Data-Frame)\n",
    "- [Working With A Dataset](#Working-With-A-Dataset)\n",
    "- [Eval and Query](#eval-and-query)\n",
    "- [Challenges](#Challenges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation\n",
    "\n",
    "The following shows different ways to create your series and data frames. If you're familiar with pandas, I suggest moving down to the `Working With A Dataset` section. However, its important to be able to easily create your objects, that way its as quick and painless to start using the full power of Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Instantiation-Series'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "One-dimensional ndarray with axis labels. Multiple series act as the columns that make up a data frame.\n",
    "\n",
    "Full documentation on series available [here](https://pandas.pydata.org/pandas-docs/version/0.24/reference/series.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(['a','b','c','d'])\n",
    "series = pd.Series(data)\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: That the indexes are provided by default_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation From Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'a' : 0, 'b' : 1, 'c' : 2}\n",
    "series = pd.Series(data)\n",
    "\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the `key`s in our `Dictionary` act as the indicies in our newly created `Series`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Accessing-Data-Series'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be accessed much like an array or list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing works as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well does accessing by key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's plenty of other things we could go over, but there are better resources online for these generic topics. Check the `References` section at the end for what I used to write this, if you want to learn more or go over more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=Instantiation-Data-Frame></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe\n",
    "\n",
    "A data frame can be thought of as a collection of series. They are the main object you'll interact with when using Pandas.\n",
    "\n",
    "Full documentation on data frame available [here](https://pandas.pydata.org/pandas-docs/version/0.24/reference/frame.html)\n",
    "\n",
    "To begin working with Pandas, you'll first need to create a data frame. There are a number of ways to accomplish this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation From A List\n",
    "\n",
    "You can create a data frame from a list of lists to indicate the separation of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While its not required to name the columns at creation, I'd suggest doing so like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
    "df = pd.DataFrame(data, columns=[\"Name\", \"Age\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you provide the column names, pandas will store them as the headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation From A Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like creating a series from a dictionary, you can do the same with a data frame. You can even set the indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}\n",
    "df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation From Series\n",
    "\n",
    "You can even create a data frame from series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_one = pd.Series([1, 2, 3]) \n",
    "series_two = pd.Series([4, 5, 6])\n",
    "\n",
    "df = pd.DataFrame([series_one, series_two])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's how you'd do it with column titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_one = pd.Series([1, 2, 3]) \n",
    "series_two = pd.Series([4, 5, 6])\n",
    "\n",
    "df = pd.DataFrame([series_one, series_two], columns=[\"First Column Title\", \"Second Column Title\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a SQL query, targeting a SQL database\n",
    "\n",
    "The snippet of code has purposefully been set to a `raw` cell, as it will fail to run. _(I was unable to setup a SQL instance for us to play with in time)_\n",
    "\n",
    "You can create a data frame from a SQL query. In my exampel below, I'd use `sqlalchemy` to communicate with my remote SQL database."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "USER = \"some_fake_user_name\"\n",
    "PASSWORD = \"some_fake_pasword\"\n",
    "HOST = \"some_fake_host\"\n",
    "DATABASE = \"some_fake_database\"\n",
    "QUERY = \"SELECT * FROM users\"\n",
    "\n",
    "ENGINE = create_engine(f\"mysql://{USER}:{PASSWORD}@{HOST}/{DATABASE}\", echo=False)\n",
    "df = pd.read_query(QUERY, con=ENGINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a CSV or other flat file\n",
    "\n",
    "One of the most frequent ways you'll load smaller amounts of data is through parsing a flat file, generally a `csv`. This is very easy to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"train.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Accessing-Data-Data-Frame'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Data\n",
    "\n",
    "We'll go more in depth in the section `Working With A Dataset`, but some useful commands are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If-Then\n",
    "\n",
    "Here well perform a single conditional on every row in the table and write back some value to the data frame based on that conditional. \n",
    "\n",
    "We could create a new dummy column that's just a bool for those who are older the 35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Age > 35.0, 'Older Than 35'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selection\n",
    "\n",
    "We can grab only rows that are of interest of us. Lets look at only individuals that survived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Survived == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorting By Multiple Columns\n",
    "\n",
    "We can sort by first `Age` and then `Survival`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[\"Age\", \"Survived\"], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working With Missing Data\n",
    "\n",
    "Often times your dataset will not be cleaned, so you'll have to investigate which columns have missing data and decide how to handle it. Some models can accept missing data and should work as is, but often times applying a technique to these missing values _may_ increase your performance.\n",
    "\n",
    "I often peak at `info`, as if it describes a column as `non-null`, than you know there's a value in every row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had missing data, the functions `dropna`,  `fillna` and `isna` can be very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iterating\n",
    "\n",
    "Sometimes you want to iterate through every row. While this generally isn't a good habbit to get into as your data size increases, you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in list(df.iterrows())[0:5]:\n",
    "    # here we can access the rows we're interested in\n",
    "    print(f\"{row.Name} is {row.Age} years old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Working-With-A-Dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With A Dataset\n",
    "\n",
    "This section will involve a mixture of code you can run to see how it works and challenges that will prompt you to do a specific task. I may not have explained how to do something, so give it a try and google things you don't know. Failing to answer the challenge correctly should give you a hint. Part of this section is to get you comfortable with googling something you haven't done before and generally looking at the Pandas documentation or a stack overflow post for the answer, as no one hour session will prepare you for anything.\n",
    "\n",
    "Challenges will be denoted in bold and can be run to get a hint.\n",
    "\n",
    "Lets recreate our dataframe by loading in the titanic training data set and walk through how we can use Pandas to interact with this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by creating our dataframe from our input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"train.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get in the habbit of inspecting your data frame as early as possible. I generally use four functions to get an idea of my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function's output should show you:\n",
    "\n",
    "- how many rows\n",
    "- how many columns\n",
    "- each column's\n",
    "  - name\n",
    "  - whether or not they are null\n",
    "  - the underlying type (sometimes Pandas can get this wrong, and you can set the column's type by looking into the `dtype` of the column)\n",
    "- how much memory is used by this data frame (data frames can use upwards for four to five times the data set's size in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will walk through some interesting calculations for each column. While some data sets can be to large to visualize with graphs, the `describe` method can help you visualize what the data will look like by reading the `mean`, `std` and `median` also known as the `50%` of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generally like to look at a few rows of the data, to just see if anything jumps out at me. For example, in the past I did not realize the encoding of my `String`s for a column were completly wrong. If I were to try to do any selection or filter logic based on a `String`, I would have gotten 0 results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets generate a scatter matrix!\n",
    "\n",
    "_Note, this might take upwards of a minute to compute on older hardware. Sorry!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df, alpha=0.2, figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I bet some would argue how useful the above graph is, but I find it to be useful for smaller data sets. This can be a handy way for looking at every row against every other row. Note that the center is where each column meets itself, so a histogram is displayed. Each side of the histogram is just a mirror image of the other side.\n",
    "\n",
    "The usefulness of this graph greatly depends on how many columns you have. If your working with a large dataset, I would probably recommend not even running the above command, as it can take quite a while.\n",
    "\n",
    "_Note: we can see that hte first column is the `id` and looks evenly distributed. This type of column should **not** be in a graph like this, as any correlation to the data shoulnd't be related, as ids **should** be assigned seemingly randomly. Sometimes and `id` column can provide value, as the `id` can have some indication of when the row was added, if there wasn't any `date` like column._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a general idea about the data set as a whole, I'll generally pick a few columns of interest and explore them specifically. Lets take `Pclass` as an example. Say we wanted to know the unique values in this column, instead of writing a custom python function, we can instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Pclass.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the data represented in the output above can somewhat be understood from `info` and `describe`, getting the whole unique listing can be useful. Here we can see that `Pclass` describes which class the ticket purchaser was in and there were three classes, `1`, `2` and `3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval-and-query'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `eval` and `query`\n",
    "\n",
    "By special request, I will be talking about the two operations in Pandas that can help you squeeze out performance when working with data frames. Lets first look at how we'd normally sum two data frames and check how long that it takes.\n",
    "\n",
    "First we'll make the random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "x = rng.rand(1000)\n",
    "y = rng.rand(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add the two datagrames using list comprehension, that most Python developers are used to doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)), dtype=x.dtype, count=len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note of how long it took per loop above.\n",
    "\n",
    "Now we can execute an logically equivalent statement, but this will utilize vector operations, which are generally much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This same idea can be applied to other operations using `eval` and `query`. Lets take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`eval`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.eval.html)\n",
    "\n",
    "The `eval` function will evaluate a str, parsing it and applying any of the following operations\n",
    "\n",
    "> +, -, *, /, **, %, //, |, &, ~\n",
    "\n",
    "First we'll generate four data frames to use for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 100000, 100\n",
    "rng = np.random.RandomState(42)\n",
    "df1, df2, df3, df4 = (pd.DataFrame(rng.rand(nrows, ncols))\n",
    "                      for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll sum our data frames again using a normal approach that most would take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit df1 + df2 + df3 + df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the time it took per loop above. \n",
    "\n",
    "Now we'll levarage the power of `eval` to perform a logically equivalent procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pd.eval('df1 + df2 + df3 + df4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above speed shuld be quicker. In demonstrations I've seen online, they've experienced a much faster speedup. What's not displayed here is a smaller memory foot print as well.\n",
    "\n",
    "When doing this on my machine, the speedup seemed miniscule at best, but don't let that fool you. Its a more performant technique, that gets only better as the amount of data increases. There are a lot more uses for `eval`, but this is merely an introduction. Please check the references for additional learning material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`query`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html)\n",
    "\n",
    "`query` is a Pandas function to operate on columns. Its also used to increase performance and decrease one's memory footprint. First lets generate some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rng.rand(1000, 3), columns=['A', 'B', 'C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say we were interested only in rows whose `A` value was less than `0.5` and whose `B` value was less than `0.5`. To do this with Pandas, one might run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit result1 = df[(df.A < 0.5) & (df.B < 0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logically equivalent operation with `query` would look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit result2 = df.query('A < 0.5 and B < 0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main take away is these operations should be used when you're considering your memory usage and cpu usage of a big operation, especially when you have a lot data. \n",
    "\n",
    "The above operation using `eval` and `query` would be much quicker, if the data we were working with was larger and more closely sized to the amount of RAM we had available. This is due to the reduced memory usage of the operations, allowing our computations to exist in RAM, where other attempts might invoke swap which is a performance nightmare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Challenges\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "Below begins the challenges that you should work in a group to complete. Each challenge may have many ways to come to the conclusion, all of which _should_ be accepted. If my `validation` functions break, please let me know, as I intended for it to be as problem free as possible for an individual.\n",
    "\n",
    "If you're stuck, simply run the cell to fell the validation and get back some hint text to help you out!\n",
    "\n",
    "The next cell will reset the object `df`, so its the original train set. Please don't modify `df`, so you can use it for each challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"train.csv\"\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 0: Let's start by trying to get the first row of the data frame. The type will also be a data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_of_df = None # REPLACE None WITH YOUR ANSWER\n",
    "\n",
    "validate_first_row_of_df(first_row_of_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 1: Create a new data frame of all individuals that had a fare of > 30.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_greater_than_30_df = None # REPLACE None WITH YOUR ANSWER\n",
    "\n",
    "validate_fare_greater_than_30(fare_greater_than_30_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 2: Calculate the sum of everyone's age in the data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_everyones_age_int = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_everyones_age(sum_of_everyones_age_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 3: Can you do the above challenge a different way?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_everyones_age_int = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_everyones_age(sum_of_everyones_age_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 4: Create a series of only the column `Cabin`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_series = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_only_cabin_series(cabin_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 5: Get the value at the first row, column `Sex`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_value_of_sex_column_first_row(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 6: What is the minimum age of the data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_age = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_min_age(min_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 7: What is the maximum age of the data set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_age = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_max_age(max_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 8: Create a pivote table. The `Pclass` column should be your index, while the `Sex` column should be your column and the summed `Survived` column should be your value. What take aways do you have from this data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_pivot_table_pclass_sex_survived(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 9: Create another pivot table, like above, and just replace the summed `Survived` with mean `Fare`. What take aways do you have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_pivot_table_pclass_sex_fare(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 10: Inverse the data frame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_df = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_transpose(transposed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 11: Remove the `Age` column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_age = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_age_removed(df_without_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 12: Rename the `Age` column to `How Old`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_renamed_df = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_age_column_renamed_to_how_old(df_age_renamed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 13: Select only the bottom 5 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_five_rows_df = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_last_five_rows(last_five_rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 14: Select the `Age` and `Sex` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_and_sex_columns_df = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_only_age_and_sex_columns(age_and_sex_columns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 15: Select only Males who are older than 50 and in Pclass 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "males_over_50_pclass_3 = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_males_over_50_in_pclass_3(males_over_50_pclass_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 16: Add a new column called `Wealthy` and set it to `True` for those in `Pclass` == 3 and `False` for all others**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_wealthy_column(x):\n",
    "    pass # REPLACE ME WITH SOME LOGIC AND USE THIS FUNCTION\n",
    "\n",
    "new_df_with_wealthy_column = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_new_wealthy_column(new_df_with_wealthy_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three challenges have no validation functions, as I ran out of time and wasn't able to implement them, as they're of type `matplotlib.axes._subplots.AxesSubplot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 17: Create a historgram of the `Age` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_histogram = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "age_histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 18: Create a bar plot of the `Age` column**\n",
    "\n",
    "_Note, this might take upwards of a minute to compute on older hardware. Sorry!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bar_plot = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "age_bar_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 19: Create a box plot of the `Age` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_box_plot = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "age_box_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next three challenges, I recommend the following [link](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) as a resource. I have to look this up **every** time I attempt to do any of these operations.\n",
    "\n",
    "I'll also generate some data for us to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 10, 3\n",
    "rng = np.random.RandomState(42)\n",
    "df1, df2 = (pd.DataFrame(rng.rand(nrows, ncols))\n",
    "                      for i in range(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 20: Append `df2` to the end of `df1`. Pleae don't perserve indexes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_df = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_append(appended_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 21: Write the data frame to a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_return_value = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_csv_output(csv_return_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 21: Write the data frame to a json file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_return_value = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_json_output(json_return_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge 22: Write the data frame to a html file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_return_value = None # REPLACE none WITH YOUR ANSWER\n",
    "\n",
    "validate_html_output(html_return_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements?\n",
    "\n",
    "If there's something I can improve, please either reach out to me, or feel free to submit a pull request.\n",
    "\n",
    "Additionally, if you have completed the notebook in the allotted time, feel free to come up with your own challenges and let me know about them or submit a pull request to [here](https://github.com/GregHilston/ds_pandas_presentation) and I'll gladly add them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://pandas.pydata.org/pandas-docs/stable/getting_started/tutorials.html\n",
    "  - https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "  - https://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html#cookbook\n",
    "  - http://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/03.12-performance-eval-and-query.html\n",
    "- https://www.tutorialspoint.com/python_pandas/\n",
    "- https://github.com/TiesdeKok/LearnPythonforResearch/blob/master/2_handling_data.ipynb\n",
    "- https://medium.com/dunder-data/minimally-sufficient-pandas-a8e67f2a2428\n",
    "- https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_meetup_pandas",
   "language": "python",
   "name": "ds_meetup_pandas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
